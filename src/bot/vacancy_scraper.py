import asyncio
import logging
import re
import json
import random
import time
from typing import Dict, List, Optional
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import quote_plus, urlparse
import html
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º AI —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
from ai_service import ai

logger = logging.getLogger(__name__)

class AvitoScraper:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å Avito"""
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        
        # –†–µ–≥–∏–æ–Ω—ã Avito
        self.regions = {
            '–ú–æ—Å–∫–≤–∞': 'moskva',
            '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 'sankt-peterburg',
            '–ö–∞–∑–∞–Ω—å': 'kazan',
            '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 'novosibirsk',
            '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 'ekaterinburg',
        }
        
        # –°–µ—Å—Å–∏—è aiohttp
        self.session = None
        
    async def init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
            connector = aiohttp.TCPConnector(ssl=False)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
            
    async def close_session(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
            self.session = None
    
    def _get_random_headers(self) -> Dict:
        """–°–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
            'Referer': 'https://www.avito.ru/'
        }
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        text = html.unescape(text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    async def scrape_avito_vacancies(self, query: str, city: str = None, limit: int = 5) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        """
        logger.info(f"üîç –ü–∞—Ä—Å–∏–º –†–ï–ê–õ–¨–ù–´–ï –≤–∞–∫–∞–Ω—Å–∏–∏ Avito: '{query}' –≤ –≥–æ—Ä–æ–¥–µ '{city}'")
        
        try:
            await self.init_session()
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            vacancies = await self._scrape_real_avito_vacancies(query, city, limit)
            
            if vacancies:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(vacancies)} –†–ï–ê–õ–¨–ù–´–• –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –≤–∞–∫–∞–Ω—Å–∏—é —á–µ—Ä–µ–∑ DeepSeek
                for vacancy in vacancies:
                    try:
                        prompt = self._create_analysis_prompt(vacancy, query)
                        ai_analysis = await ai.analyze_vacancy_with_ai(prompt)
                        vacancy['ai_analysis'] = self._format_ai_analysis(ai_analysis, vacancy)
                        await asyncio.sleep(0.5)
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
                        vacancy['ai_analysis'] = self._get_default_analysis(vacancy)
                
                return vacancies
            else:
                logger.warning("‚ùå –†–µ–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ - –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–µ–π–∫–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏!
                return []
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Avito: {e}", exc_info=True)
            return []
    
    async def _scrape_real_avito_vacancies(self, query: str, city: str = None, limit: int = 5) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å –ö–û–ù–ö–†–ï–¢–ù–´–ú–ò —Å—Å—ã–ª–∫–∞–º–∏"""
        try:
            region_slug = self.regions.get(city, 'rossiya')
            query_encoded = quote_plus(query)
            
            # URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π
            url = f"https://www.avito.ru/{region_slug}/vakansii?q={query_encoded}"
            
            logger.info(f"üîó –ü–∞—Ä—Å–∏–º URL –ø–æ–∏—Å–∫–∞: {url}")
            
            headers = self._get_random_headers()
            
            async with self.session.get(url, headers=headers, ssl=False) as response:
                if response.status == 200:
                    html_content = await response.text()
                    return self._extract_real_vacancies_with_links(html_content, query, city, limit)
                else:
                    logger.warning(f"HTTP —Å—Ç–∞—Ç—É—Å: {response.status}")
                    return []
                        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
            return []
    
    def _extract_real_vacancies_with_links(self, html_content: str, query: str, city: str, limit: int) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¢–û–õ–¨–ö–û –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –†–ï–ê–õ–¨–ù–´–ú–ò —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        –§–æ—Ä–º–∞—Ç —Å—Å—ã–ª–∫–∏: https://www.avito.ru/moskva/vakansii/NAZVANIE_ID
        –ò–õ–ò https://www.avito.ru/moskva/vakansii/ID
        """
        vacancies = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            items = soup.select('[data-marker="item"]')
            
            if not items:
                items = soup.select('.iva-item-content-rejJg')
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(items)} –∫–∞—Ä—Ç–æ—á–µ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            for item in items[:limit]:
                try:
                    # === 1. –ò—â–µ–º –†–ï–ê–õ–¨–ù–£–Æ —Å—Å—ã–ª–∫—É –Ω–∞ –ö–û–ù–ö–†–ï–¢–ù–û–ï –æ–±—ä—è–≤–ª–µ–Ω–∏–µ ===
                    link_elem = None
                    url = None
                    
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–∫–∏
                    selectors = [
                        'a[href*="/vakansii/"][href*="_"]',  # —Å—Å—ã–ª–∫–∞ —Å ID
                        'a[href*="/vakansii/"][href$="/"]',   # —Å—Å—ã–ª–∫–∞ —Å —Å–ª–µ—à–µ–º –≤ –∫–æ–Ω—Ü–µ
                        'a[data-marker="item-title"]',        # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–∞—Ä–∫–µ—Ä
                        'a[href*="/vakansii/"]'              # –ª—é–±–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏
                    ]
                    
                    for selector in selectors:
                        link_elem = item.select_one(selector)
                        if link_elem and link_elem.get('href'):
                            href = link_elem['href']
                            
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                            if href.startswith('//'):
                                url = f"https:{href}"
                            elif href.startswith('/'):
                                url = f"https://www.avito.ru{href}"
                            else:
                                url = href
                            
                            # === 2. –ü–†–û–í–ï–†–Ø–ï–ú, –ß–¢–û –≠–¢–û –°–°–´–õ–ö–ê –ù–ê –ö–û–ù–ö–†–ï–¢–ù–û–ï –û–ë–™–Ø–í–õ–ï–ù–ò–ï ===
                            # –£ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –µ—Å—Ç—å ID –≤ –∫–æ–Ω—Ü–µ —Å—Å—ã–ª–∫–∏
                            has_id = bool(re.search(r'/\d+$', href)) or bool(re.search(r'_\d+$', href))
                            
                            if has_id:
                                # –≠—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ!
                                break
                            else:
                                # –≠—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–ª–∏ –ø–æ–∏—Å–∫ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                url = None
                                continue
                    
                    if not url:
                        continue
                    
                    # === 3. –ò–ó–í–õ–ï–ö–ê–ï–ú ID –û–ë–™–Ø–í–õ–ï–ù–ò–Ø ===
                    item_id = None
                    id_match = re.search(r'/(\d+)$', href)
                    if id_match:
                        item_id = id_match.group(1)
                    else:
                        id_match = re.search(r'_(\d+)$', href)
                        if id_match:
                            item_id = id_match.group(1)
                    
                    if not item_id:
                        # –ù–µ—Ç ID - —ç—Ç–æ –Ω–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                        continue
                    
                    # === 4. –ó–ê–ì–û–õ–û–í–û–ö ===
                    title_elem = item.select_one('[itemprop="name"], h3, [data-marker="item-title"]')
                    if not title_elem:
                        title_elem = item.select_one('.title-root-zZCwT')
                    
                    title = self._clean_text(title_elem.get_text(strip=True)) if title_elem else None
                    if not title:
                        continue
                    
                    # === 5. –ó–ê–†–ü–õ–ê–¢–ê ===
                    price_elem = item.select_one('[data-marker="item-price"]')
                    if not price_elem:
                        price_elem = item.select_one('.price-price-JP7qe')
                    
                    salary = self._clean_text(price_elem.get_text(strip=True)) if price_elem else "–î–æ–≥–æ–≤–æ—Ä–Ω–∞—è"
                    
                    # === 6. –û–ü–ò–°–ê–ù–ò–ï ===
                    desc_elem = item.select_one('[data-marker="item-specific-params"]')
                    if not desc_elem:
                        desc_elem = item.select_one('.iva-item-description-StepN')
                    
                    description = self._clean_text(desc_elem.get_text(strip=True)) if desc_elem else ""
                    
                    # === 7. –ì–û–†–û–î ===
                    city_elem = item.select_one('[data-marker="item-address"]')
                    if not city_elem:
                        city_elem = item.select_one('.geo-georeferences-SEtee')
                    
                    city_name = self._clean_text(city_elem.get_text(strip=True)) if city_elem else (city or "–ù–µ —É–∫–∞–∑–∞–Ω")
                    
                    # === 8. –î–ê–¢–ê ===
                    date_elem = item.select_one('[data-marker="item-date"]')
                    date_text = self._clean_text(date_elem.get_text(strip=True)) if date_elem else "–ù–µ–¥–∞–≤–Ω–æ"
                    
                    # === 9. –°–û–ó–î–ê–ï–ú –í–ê–ö–ê–ù–°–ò–Æ –¢–û–õ–¨–ö–û –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò ===
                    vacancy = {
                        'source': 'avito',
                        'source_id': f"av_{item_id}",
                        'title': title[:200],
                        'description': description[:500] if description else '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ Avito',
                        'salary': salary,
                        'city': city_name,
                        'contacts': '–ö–æ–Ω—Ç–∞–∫—Ç—ã –Ω–∞ Avito',
                        'requirements': self._extract_requirements(description),
                        'url': url,  # –≠–¢–û –†–ï–ê–õ–¨–ù–ê–Ø –°–°–´–õ–ö–ê –ù–ê –ö–û–ù–ö–†–ï–¢–ù–û–ï –û–ë–™–Ø–í–õ–ï–ù–ò–ï!
                        'date': date_text,
                        'query': query,
                        'item_id': item_id,
                        'is_real': True,
                        'has_real_link': True
                    }
                    
                    logger.info(f"‚úÖ –ù–ê–ô–î–ï–ù–ê –†–ï–ê–õ–¨–ù–ê–Ø –í–ê–ö–ê–ù–°–ò–Ø: {title}")
                    logger.info(f"   üîó –°–°–´–õ–ö–ê: {url}")
                    logger.info(f"   üÜî ID: {item_id}")
                    
                    vacancies.append(vacancy)
                    
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
        
        return vacancies
    
    def _create_analysis_prompt(self, vacancy: Dict, user_query: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç—É —Ä–µ–∞–ª—å–Ω—É—é –≤–∞–∫–∞–Ω—Å–∏—é —Å Avito:

–î–æ–ª–∂–Ω–æ—Å—Ç—å: {vacancy.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
–ó–∞—Ä–ø–ª–∞—Ç–∞: {vacancy.get('salary', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}
–ì–æ—Ä–æ–¥: {vacancy.get('city', '–ù–µ —É–∫–∞–∑–∞–Ω')}
–û–ø–∏—Å–∞–Ω–∏–µ: {vacancy.get('description', '')[:300]}

–û—Ü–µ–Ω–∏ –ø–æ 100-–±–∞–ª–ª—å–Ω–æ–π —à–∫–∞–ª–µ:
1. –ù–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∫–∞–Ω—Å–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—Å—É "{user_query}"
2. –ù–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞
3. –°–≤–µ–∂–µ—Å—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏
4. –ü–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "score": —á–∏—Å–ª–æ_–æ—Ç_0_–¥–æ_100,
    "recommendation": "–∫—Ä–∞—Ç–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è",
    "key_points": ["–ø–ª—é—Å1", "–ø–ª—é—Å2", "–ø–ª—é—Å3"]
}}
"""
        return prompt
    
    def _format_ai_analysis(self, ai_analysis: Dict, vacancy: Dict) -> Dict:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ –æ—Ç AI"""
        score = ai_analysis.get('score', 70)
        
        if score >= 80:
            color = "üü¢"
            emoji = "üî•"
        elif score >= 65:
            color = "üü°"
            emoji = "‚úÖ"
        elif score >= 50:
            color = "üü†"
            emoji = "‚ö†Ô∏è"
        else:
            color = "üî¥"
            emoji = "‚ùå"
        
        key_points = ai_analysis.get('key_points', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ
        salary = vacancy.get('salary', '')
        if '000' in salary:
            key_points.insert(0, f"üí∞ {salary}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–≤–µ–∂–µ—Å—Ç–∏
        date = vacancy.get('date', '').lower()
        if '—Å–µ–≥–æ–¥–Ω—è' in date:
            key_points.insert(0, "üïí –°–≤–µ–∂–∞—è –≤–∞–∫–∞–Ω—Å–∏—è")
        
        return {
            "compatibility_score": score,
            "recommendation": ai_analysis.get('recommendation', '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–∫–∞–Ω—Å–∏—é'),
            "color": color,
            "emoji": emoji,
            "key_points": key_points[:3],
            "is_real": True
        }
    
    def _get_default_analysis(self, vacancy: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "compatibility_score": 70,
            "recommendation": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–∫–∞–Ω—Å–∏—é –ø–æ —Å—Å—ã–ª–∫–µ",
            "color": "üü°",
            "emoji": "‚úÖ",
            "key_points": [
                f"üìç {vacancy.get('city', '–ì–æ—Ä–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω')}",
                f"üíº {vacancy.get('title', '')[:30]}...",
                "üîó –†–µ–∞–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Avito"
            ],
            "is_real": True
        }
    
    def _extract_requirements(self, description: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è"""
        if not description:
            return ["–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞ Avito"]
        
        requirements = []
        sentences = re.split(r'[.!?]+', description)
        
        for sentence in sentences[:2]:
            if any(word in sentence.lower() for word in ['—Ç—Ä–µ–±–æ–≤–∞–Ω', '–æ–±—è–∑–∞–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º']):
                clean = self._clean_text(sentence)
                if clean and len(clean) > 10:
                    requirements.append(clean[:100])
        
        if not requirements:
            requirements = ["–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞ Avito"]
        
        return requirements[:2]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
scraper = AvitoScraper()

async def test_scraper():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∏ –≤–µ–¥—É—Ç –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
    print("üîç –¢–ï–°–¢: –ü–æ–∏—Å–∫ –†–ï–ê–õ–¨–ù–´–• –≤–∞–∫–∞–Ω—Å–∏–π —Å –ö–û–ù–ö–†–ï–¢–ù–´–ú–ò —Å—Å—ã–ª–∫–∞–º–∏")
    print("=" * 60)
    
    await scraper.init_session()
    
    test_cases = [
        ("–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä", "–ú–æ—Å–∫–≤–∞"),
        ("–±–∞—Ä–∏—Å—Ç–∞", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"),
    ]
    
    for query, city in test_cases:
        print(f"\nüìå –ü–æ–∏—Å–∫: '{query}' –≤ '{city}'")
        print("-" * 50)
        
        vacancies = await scraper.scrape_avito_vacancies(query, city, limit=3)
        
        if vacancies:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –†–ï–ê–õ–¨–ù–´–• –≤–∞–∫–∞–Ω—Å–∏–π: {len(vacancies)}")
            
            for i, vac in enumerate(vacancies, 1):
                print(f"\n  {i}. {vac['title']}")
                print(f"     üí∞ {vac['salary']}")
                print(f"     üìç {vac['city']}")
                print(f"     üîó {vac['url']}")
                print(f"     üÜî ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {vac.get('item_id', '–ù–µ—Ç ID')}")
                print(f"     üìÖ {vac['date']}")
                print(f"     ‚úÖ –≠—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –ö–û–ù–ö–†–ï–¢–ù–û–ï –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {'–î–ê' if vac.get('has_real_link') else '–ù–ï–¢'}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Å—Å—ã–ª–∫–∏
                if vac['url']:
                    if re.search(r'/vakansii/\d+$', vac['url']) or re.search(r'/vakansii/.*_\d+$', vac['url']):
                        print(f"     ‚úÖ –§–û–†–ú–ê–¢ –°–°–´–õ–ö–ò –ö–û–†–†–ï–ö–¢–ù–´–ô")
                    else:
                        print(f"     ‚ùå –§–û–†–ú–ê–¢ –°–°–´–õ–ö–ò –ù–ï–ö–û–†–†–ï–ö–¢–ù–´–ô")
        else:
            print("‚ùå –†–µ–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            print("   Avito –º–æ–≥ –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥")
        
        await asyncio.sleep(1)
    
    await scraper.close_session()
    print("\n" + "=" * 60)

if __name__ == "__main__":
    asyncio.run(test_scraper())